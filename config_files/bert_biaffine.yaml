run: 
  cuda: true
  cpu: false
  seed: 1234
  #执行模型：train,dev,inference
  run_mode: 'train'
  max_steps: 500000
  max_train_epochs: 500
  eval_interval: 100
  early_stop: true
  early_stop_steps: 5000
data_set: 
  data_dir: 'dataset'
  train_file: 'train/sdp_text_train.conllu'
  dev_file: 'dev/sdp_text_dev.conllu'
  test_file: 'test/sdp_text_test.conllu'
  per_gpu_train_batch_size: 20
  per_gpu_eval_batch_size: 10
  skip_too_long_input: true
output:
  saved_model_dir: 'checkpoints'
  save_name: 'bert'
  dev_output_file: 'bert_dev.txt'
graph_vocab: 
  #依存弧的vocab，必须提前生成
  graph_vocab_file: 'dataset/graph_vocab.txt'
encoder: 
  #encoder的类型：bert,transformer,lstm ....
  encoder_type: 'bert'
  encoder_output_dim: 768
CharRNN: 
Transformer: 
BERT: 
  #bert 模型的文件路径
  bert_path: /data/private/lhy/data/bert_pretrained_model/bert-base-chinese/
  #最大长度 必须超过数据集的最大长度（字数）,256比较合适，新闻领域的最大句长可达233
  max_seq_len: 100
  #ROOT的表示形式：unused,cls,root ....
  root_representation: 'unused'
  #中文单词的提取方式：s,e,s+e,s-e
  bert_chinese_word_select: 's+e'
  #BERT输出的选择方式：last,last_four_sum,last_four_cat,all_sum
  bert_output_mode: 'last'
decoder: 
  biaffine_hidden_dim: 300
  biaffine_dropout: 0.1
update: 
  learned_loss_ratio: true
  label_loss_ratio: 0.5
  max_grad_norm: 1.0
  optimizer: 'adamw'
  beta1: 0
  beta2: 0.95
  L2_penalty: 3.0e-9
  eps: 1.0e-12
  weight_decay: 0.0
  learning_rate: 5.0e-5
  adam_epsilon: 1.0e-8
  warmup_steps: 0
