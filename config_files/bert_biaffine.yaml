# yaml 格式说明：
# （1）注意空格！yaml格式非常严格。双空格开头，冒号后必须有一个空格
# （2）字符串用单引号引起来
# （3）浮点数或者科学计数法必须用小数点（否则可能被当做字符串）
# （4）布尔类型：true，false
run:
  cuda: true
  cpu: false
  seed: 1234
  #执行模型：train,dev,inference
  run_mode: 'train'
  #只需要指定最大的epoch数量，不需要指定最大steps
  #可能的选择：80,150
  max_train_epochs: 80
  eval_interval: 100
  early_stop: false
  early_stop_epochs: 6
data_set:
  data_dir: 'dataset'
  train_file: 'train/text_news.train.conllu'
  dev_file: 'dev/sdp_text_dev.conllu'
  test_file: 'test/sdp_text_test.conllu'
  # GPU 32GB:80; 24GB:64; 12GB:32; 10GB:20
  per_gpu_train_batch_size: 20
  # GPU <=12GB:10; >12GB:20或者30
  per_gpu_eval_batch_size: 10
  skip_too_long_input: true
output:
  output_dir: 'output'
  log_name: 'parser'
  save_best_model: false
graph_vocab:
  #依存弧的vocab，必须提前生成
  graph_vocab_file: 'dataset/graph_vocab.txt'
encoder: 
  #encoder的类型：bert,transformer,lstm ....
  encoder_type: 'bert'
  encoder_output_dim: 768
CharRNN: 
Transformer: 
BERT: 
  #bert 模型的文件路径
  bert_path: '/data/private/lhy/data/bert_pretrained_model/bert-base-chinese/'
  #最大长度 必须超过数据集的最大长度（字数）,新闻领域的最大句长可达233
  max_seq_len: 100
  #ROOT的表示形式：unused,cls,root ....
  root_representation: 'unused'
  #中文单词的提取方式：s,e,s+e,s-e
  bert_chinese_word_select: 'e'
  #BERT输出的选择方式：last,last_four_sum,last_four_cat,all_sum,attention
  bert_output_mode: 'last_four_sum'
  bert_after: 'transformer'
  after_layers: 2
  after_dropout: 0.2
decoder: 
  biaffine_hidden_dim: 600
  biaffine_dropout: 0.33
  direct_biaffine: false
update: 
  learned_loss_ratio: true
  label_loss_ratio: 0.5
  scale_loss: false
  loss_scaling_ratio: 2
  label_smoothing: 0.03
  max_grad_norm: 5.0
  # adam-bert (huggingface版本的adamw); adamw-torch (torch 1.2); adam;
  optimizer: 'adamw-bert'
  beta1: 0.9
  beta2: 0.99
  eps: 1.0e-12
  weight_decay: 3.0e-9
  learning_rate: 5.0e-5
  # bertDistill:1.0e-6;
  adam_epsilon: 1.0e-8
  # bertDistill:0.05;
  warmup_prop: 0.05
  average_loss_by_words_num: true
